name: Training
inputs:
- {name: file, type: parquet}
outputs:
- {name: Output, type: String}
implementation:
  container:
    image: burntt/nby-cogflow-informer:latest
    command:
    - sh
    - -ec
    - |
      program_path=$(mktemp)
      printf "%s" "$0" > "$program_path"
      python3 -u "$program_path" "$@"
    - |
      def training(file_path):
          import sys
          import os
          import pandas as pd
          import torch
          import numpy as np
          import shutil

          # Log the system path for debugging
          print("System path before appending directories:")
          print(sys.path)
          
          # Add directories to the system path
          base_dir = os.path.dirname(file_path)
          sys.path.append(os.path.join(base_dir, 'exp'))
          sys.path.append(os.path.join(base_dir, 'models'))
          sys.path.append(os.path.join(base_dir, 'utils'))
          sys.path.append(os.path.join(base_dir, 'data'))

          # Log the system path after appending directories
          print("System path after appending directories:")
          print(sys.path)

          # Log the contents of each directory for debugging
          directories_to_check = ['exp', 'models', 'utils', 'data']
          for directory in directories_to_check:
              full_path = os.path.join(base_dir, directory)
              if os.path.exists(full_path):
                  print(f"Contents of {full_path}:")
                  print(os.listdir(full_path))
              else:
                  print(f"Directory {full_path} does not exist.")

          # Attempt to import the required module
          try:
              from exp.exp_informer import Exp_Informer
              print("Import successful.")
          except ModuleNotFoundError as e:
              print(f"ModuleNotFoundError: {e}")
              return "Module import failed"

          Exp = Exp_Informer

          cf.autolog()
          cf.pytorch.autolog()
          experiment_id = cf.set_experiment(
              experiment_name="Custom Model Informer Time-Series",
          )
          with cf.start_run('custom_model_run_informer') as run:
              for ii in range(args.itr):
                  setting = '{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_at{}_fc{}_eb{}_dt{}_mx{}_{}_{}'.format(
                      args.model, args.data, args.features, args.seq_len, args.label_len,
                      args.pred_len, args.d_model, args.n_heads, args.e_layers,
                      args.d_layers, args.d_ff, args.attn, args.factor, args.embed,
                      args.distil, args.mix, args.des, ii
                  )

                  cf.log_param("seq_len", args.seq_len)
                  cf.log_param("n_heads", args.n_heads)
                  cf.log_param("enc_lay", args.e_layers)
                  cf.log_param("pred_len", args.pred_len)
                  cf.log_param("dec_lay", args.d_layers)

                  exp = Exp(args)
                  print('>>>>>>>start training : {}>>>>>>>>>>>>>>>>>>>>>>>>>>'.format(setting))
                  model = exp.train(setting)
                  print('>>>>>>>end training : {}>>>>>>>>>>>>>>>>>>>>>>>>>>'.format(setting))

                  print('>>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))
                  test_results = exp.test(setting)

                  cf.log_metric("mae", test_results['mae'])
                  cf.log_metric("mse", test_results['mse'])
                  cf.log_metric("rmse", test_results['rmse'])
                  cf.log_metric("r2", test_results['r2'])

                  print('>>>>>>>predicting : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))
                  preds = exp.predict(setting, True)

                  args_file_path = './args.txt'
                  with open(args_file_path, 'w') as f:
                      for arg, value in vars(args).items():
                          f.write(f"{arg}={value}\n")
                  artifacts = {"args.txt": args_file_path}

                  device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
                  model.to(device)

                  example_x_enc = torch.rand(1, args.seq_len, args.enc_in).to(device).float()
                  example_x_mark_enc = torch.rand(1, args.seq_len, 1).to(device).float()
                  example_x_dec = torch.rand(1, args.pred_len, args.dec_in).to(device).float()
                  example_x_mark_dec = torch.rand(1, args.pred_len, 1).to(device).float()
                  inputs_example = (example_x_enc, example_x_mark_enc, example_x_dec, example_x_mark_dec)
                  output_example = model(*inputs_example)

                  inputs_example_cpu = tuple(tensor.cpu().detach().numpy() for tensor in inputs_example)
                  output_example_cpu = output_example.cpu().detach().numpy()
                  inputs_example_cpu_no_batch = tuple(input_array[0] for input_array in inputs_example_cpu)
                  output_example_cpu_no_batch = output_example_cpu[0]

                  inputs_combined = np.concatenate([input_array.flatten() for input_array in inputs_example_cpu_no_batch], axis=-1)
                  input_df = pd.DataFrame(inputs_combined)

                  try:
                      signature = cf.models.infer_signature(input_df, output_example_cpu_no_batch)
                      print('Inference Signature Correctly Saved!')
                  except Exception as e:
                      print(f"Error inferring signature: {e}")
                      signature = None

                  model_info = cf.pyfunc.log_model(
                      artifact_path='informer-google-trace',
                      python_model=exp,
                      artifacts=artifacts,
                      pip_requirements=[],
                      input_example=input_df,
                      signature=signature
                  )

                  print(f"Run_id", run.info.run_id)
                  print(f"Artifact_uri", run.info.artifact_uri)
                  print(f"Artifact_path", run.info.artifact_uri)
                  registered_models_list = cf.search_registered_models()
                  print(registered_models_list)
          return f"{run.info.artifact_uri}/{model_info.artifact_path}"

      def _serialize_str(str_value: str) -> str:
          if not isinstance(str_value, str):
              raise TypeError('Value "{}" has type "{}" instead of str.'.format(
                  str(str_value), str(type(str_value))))
          return str_value

      import argparse
      _parser = argparse.ArgumentParser(prog='Training', description='')
      _parser.add_argument("--file", dest="file_path", type=str, required=True, default=argparse.SUPPRESS)
      _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
      _parsed_args = vars(_parser.parse_args())
      _output_files = _parsed_args.pop("_output_paths", [])

      _outputs = training(**_parsed_args)

      _outputs = [_outputs]

      _output_serializers = [
          _serialize_str,
      ]

      import os
      for idx, output_file in enumerate(_output_files):
          try:
              os.makedirs(os.path.dirname(output_file))
          except OSError:
              pass
          with open(output_file, 'w') as f:
              f.write(_output_serializers[idx](_outputs[idx]))
    args:
    - --file
    - {inputPath: file}
    - '----output-paths'
    - {outputPath: Output}
