name: Training
inputs:
- {name: file, type: parquet}
- {name: args}
outputs:
- {name: Output, type: String}
implementation:
  container:
    image: burntt/nby-cogflow-informer:latest
    command:
    - sh
    - -ec
    - |
      program_path=$(mktemp)
      printf "%s" "$0" > "$program_path"
      python3 -u "$program_path" "$@"
    - "def training(file_path, args):\n    import sys\n    import os\n    import pandas\
      \ as pd\n    import torch\n    import numpy as np\n    import cogflow as cf\n\
      \    import json\n\n    args = argparse.Namespace(**json.loads(args_json))\n\
      \n    # Log the system path before appending directories\n    print(\"System\
      \ path before appending directories:\")\n    print(sys.path)\n\n    # Add root\
      \ and necessary directories to sys.path\n    sys.path.append('/')\n    sys.path.append('/exp')\n\
      \    sys.path.append('/models')\n    sys.path.append('/utils')\n\n    # Log\
      \ the system path after appending directories\n    print(\"System path after\
      \ appending directories:\")\n    print(sys.path)\n\n    # Log the contents of\
      \ each directory for debugging\n    directories_to_check = ['/exp', '/models',\
      \ '/utils', '/data']\n    for directory in directories_to_check:\n        if\
      \ os.path.isdir(directory):\n            print(f\"Contents of {directory}:\"\
      )\n            print(os.listdir(directory))\n        else:\n            print(f\"\
      {directory} is not a directory or does not exist.\")\n\n    # Attempt to import\
      \ the required module\n    try:\n        from exp.exp_informer import Exp_Informer\n\
      \        print(\"Import successful.\")\n    except ModuleNotFoundError as e:\n\
      \        print(f\"ModuleNotFoundError: {e}\")\n        return \"Module import\
      \ failed\"\n\n    Exp = Exp_Informer\n\n    cf.autolog()\n    cf.pytorch.autolog()\n\
      \    experiment_id = cf.set_experiment(\n        experiment_name=\"Custom Model\
      \ Informer Time-Series\",\n    )\n    with cf.start_run('custom_model_run_informer')\
      \ as run:\n        for ii in range(args.itr):\n            setting = '{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_at{}_fc{}_eb{}_dt{}_mx{}_{}_{}'.format(\n\
      \                args.model, args.data, args.features, args.seq_len, args.label_len,\
      \ args.pred_len, args.d_model, \n                args.n_heads, args.e_layers,\
      \ args.d_layers, args.d_ff, args.attn, args.factor, args.embed, \n         \
      \       args.distil, args.mix, args.des, ii\n            )\n\n            cf.log_param(\"\
      seq_len\", args.seq_len)\n            cf.log_param(\"n_heads\", args.n_heads)\n\
      \            cf.log_param(\"enc_lay\", args.e_layers)\n            cf.log_param(\"\
      pred_len\", args.pred_len)\n            cf.log_param(\"dec_lay\", args.d_layers)\n\
      \n            exp = Exp(args)\n            print('>>>>>>>start training : {}>>>>>>>>>>>>>>>>>>>>>>>>>>'.format(setting))\n\
      \            model = exp.train(setting)\n            print('>>>>>>>end training\
      \ : {}>>>>>>>>>>>>>>>>>>>>>>>>>>'.format(setting))\n\n            print('>>>>>>>testing\
      \ : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n            test_results\
      \ = exp.test(setting)\n\n            cf.log_metric(\"mae\", test_results['mae'])\n\
      \            cf.log_metric(\"mse\", test_results['mse'])\n            cf.log_metric(\"\
      rmse\", test_results['rmse'])\n            cf.log_metric(\"r2\", test_results['r2'])\n\
      \n            print('>>>>>>>predicting : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n\
      \            preds = exp.predict(setting, True)\n\n            args_file_path\
      \ = './args.txt'\n            with open(args_file_path, 'w') as f:\n       \
      \         for arg, value in vars(args).items():\n                    f.write(f\"\
      {arg}={value}\\n\")\n            artifacts = {\"args.txt\": args_file_path}\n\
      \n            device = torch.device(\"cuda\" if torch.cuda.is_available() else\
      \ \"cpu\")\n            model.to(device)\n\n            example_x_enc = torch.rand(1,\
      \ args.seq_len, args.enc_in).to(device).float()\n            example_x_mark_enc\
      \ = torch.rand(1, args.seq_len, 1).to(device).float()\n            example_x_dec\
      \ = torch.rand(1, args.pred_len, args.dec_in).to(device).float()\n         \
      \   example_x_mark_dec = torch.rand(1, args.pred_len, 1).to(device).float()\n\
      \            inputs_example = (example_x_enc, example_x_mark_enc, example_x_dec,\
      \ example_x_mark_dec)\n            output_example = model(*inputs_example)\n\
      \n            inputs_example_cpu = tuple(tensor.cpu().detach().numpy() for tensor\
      \ in inputs_example)\n            output_example_cpu = output_example.cpu().detach().numpy()\n\
      \            inputs_example_cpu_no_batch = tuple(input_array[0] for input_array\
      \ in inputs_example_cpu)\n            output_example_cpu_no_batch = output_example_cpu[0]\n\
      \n            inputs_combined = np.concatenate([input_array.flatten() for input_array\
      \ in inputs_example_cpu_no_batch], axis=-1)\n            input_df = pd.DataFrame(inputs_combined)\n\
      \n            try:\n                signature = cf.models.infer_signature(input_df,\
      \ output_example_cpu_no_batch)\n                print('Inference Signature Correctly\
      \ Saved!')\n            except Exception as e:\n                print(f\"Error\
      \ inferring signature: {e}\")\n                signature = None\n\n        \
      \    model_info = cf.pyfunc.log_model(\n                artifact_path='informer-google-trace',\n\
      \                python_model=exp,\n                artifacts=artifacts,\n \
      \               pip_requirements=[],\n                input_example=input_df,\n\
      \                signature=signature\n            )\n\n            print(f\"\
      Run_id\", run.info.run_id)\n            print(f\"Artifact_uri\", run.info.artifact_uri)\n\
      \            print(f\"Artifact_path\", run.info.artifact_uri)\n            registered_models_list\
      \ = cf.search_registered_models()\n            print(registered_models_list)\n\
      \n    return f\"{run.info.artifact_uri}/{model_info.artifact_path}\"\n\ndef\
      \ _serialize_str(str_value: str) -> str:\n    if not isinstance(str_value, str):\n\
      \        raise TypeError('Value \"{}\" has type \"{}\" instead of str.'.format(\n\
      \            str(str_value), str(type(str_value))))\n    return str_value\n\n\
      import argparse\n_parser = argparse.ArgumentParser(prog='Training', description='')\n\
      _parser.add_argument(\"--file\", dest=\"file_path\", type=str, required=True,\
      \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--args\", dest=\"args\"\
      , type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
      ----output-paths\", dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args\
      \ = vars(_parser.parse_args())\n_output_files = _parsed_args.pop(\"_output_paths\"\
      , [])\n\n_outputs = training(**_parsed_args)\n\n_outputs = [_outputs]\n\n_output_serializers\
      \ = [\n    _serialize_str,\n\n]\n\nimport os\nfor idx, output_file in enumerate(_output_files):\n\
      \    try:\n        os.makedirs(os.path.dirname(output_file))\n    except OSError:\n\
      \        pass\n    with open(output_file, 'w') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"
    args:
    - --file
    - {inputPath: file}
    - --args
    - {inputValue: args}
    - '----output-paths'
    - {outputPath: Output}
