1
00:00:00,640 --> 00:00:04,200
Hello, I'm Berend Gort from Nearby Computing from

2
00:00:04,700 --> 00:00:08,200
the Department of AI Research and today I will be presenting

3
00:00:08,700 --> 00:00:12,760
to you a demo for the Cognitive Framework where we use

4
00:00:13,260 --> 00:00:17,840
it to deploy state of the art time series forecasters in

5
00:00:18,340 --> 00:00:21,800
a microservice. A time series forecaster is simply when you send

6
00:00:22,300 --> 00:00:25,920
it for example some CPU data and it will respond with you

7
00:00:26,420 --> 00:00:30,340
to you with some predictions. This is part of the Verge Project

8
00:00:30,580 --> 00:00:34,180
Work Package 5 Deliverable 2.3.

9
00:00:35,860 --> 00:00:37,860
So what is the Cognitive Framework?

10
00:00:38,820 --> 00:00:42,499
Here we have a scenario where we have a prediction setup with the Cognitive

11
00:00:42,500 --> 00:00:45,980
Framework used. As for

12
00:00:46,480 --> 00:00:50,020
me, I'm a machine learning engineer, but I could be working in parallel to data

13
00:00:50,520 --> 00:00:54,250
scientists and deep learning engineers or other colleagues interested in

14
00:00:54,750 --> 00:00:56,730
uploading models and serving models.

15
00:00:58,170 --> 00:01:01,490
It's possible through the CogFlow library to then through

16
00:01:01,990 --> 00:01:04,570
your coding, upload a model into the cognitive framework,

17
00:01:05,770 --> 00:01:09,770
which then uploads that to a tracking server.

18
00:01:11,130 --> 00:01:15,090
From the tracking server, experimental results are saved and

19
00:01:15,590 --> 00:01:17,850
recorded to a PostgreSQL database.

20
00:01:19,150 --> 00:01:22,750
Also at the same time the artifacts like the model weights

21
00:01:23,250 --> 00:01:26,870
are stored to an S3 bucket. If you

22
00:01:27,370 --> 00:01:31,750
want the server model, you can also do that with the

23
00:01:32,250 --> 00:01:35,830
CogFlow library where you can identify an experiment

24
00:01:36,330 --> 00:01:39,990
by looking at the experiment values and then pull a model

25
00:01:40,490 --> 00:01:41,790
from the S3 storage.

26
00:01:44,040 --> 00:01:48,000
The cognitive framework packages the model and then deploys

27
00:01:48,500 --> 00:01:50,920
it in any particular manner the user wants.

28
00:01:51,720 --> 00:01:55,480
So now we will log into the environment and here

29
00:01:55,560 --> 00:02:00,160
we arrive in the hi row overview edge micro datacenters

30
00:02:00,660 --> 00:02:04,160
where we have home with all quick shortcuts. Some notebooks

31
00:02:04,660 --> 00:02:07,160
work areas run experiments,

32
00:02:07,720 --> 00:02:12,390
experiments with k FP pipelines like

33
00:02:12,890 --> 00:02:16,270
pipeline runs and maybe some recurring runs also

34
00:02:16,350 --> 00:02:19,710
volumes tensorboards for data analytics

35
00:02:20,210 --> 00:02:23,670
and an MLflow section. And now we will go

36
00:02:24,170 --> 00:02:27,630
to the notebooks and here we find our work environment

37
00:02:28,030 --> 00:02:31,630
which is called the Nbuycock code and we will

38
00:02:31,790 --> 00:02:35,630
start the server. Now the notebook has started and

39
00:02:36,130 --> 00:02:39,390
we can connect to it. Now we will shortly go over the

40
00:02:39,890 --> 00:02:43,910
code. All of this other redundant code

41
00:02:44,410 --> 00:02:48,910
is the model and the experiment, the input data, and some

42
00:02:49,410 --> 00:02:52,350
utility functions. Here you can see the attention, decoder,

43
00:02:52,850 --> 00:02:55,990
embedder, encoder, but that's not very significant.

44
00:02:56,310 --> 00:02:59,830
What's significant is this file, run_cogflow_server.

45
00:03:00,310 --> 00:03:03,760
As you can see, it imports CogFlow as CF and

46
00:03:04,260 --> 00:03:08,320
if we control-F CF throughout the document, we use

47
00:03:08,820 --> 00:03:12,640
the CogFlow library internally throughout this experiment to

48
00:03:13,040 --> 00:03:16,640
log the experiment, to set the experiment name, to start the

49
00:03:17,140 --> 00:03:20,400
run, to log some hyperparameters of this model,

50
00:03:20,480 --> 00:03:24,440
to log some metrics. Let's see what else, and

51
00:03:24,940 --> 00:03:28,280
of course, to finally infer the model signature, which means

52
00:03:28,780 --> 00:03:32,630
that we later also know how to send inference

53
00:03:33,130 --> 00:03:37,790
requests to the model and to log the model into

54
00:03:38,290 --> 00:03:40,190
the Cognitive Framework.

55
00:03:42,590 --> 00:03:46,430
Then we have some exception, and then

56
00:03:46,510 --> 00:03:50,670
we have a couple of pipeline functions, and

57
00:03:50,910 --> 00:03:54,230
in order to show that, I'll first go to the end, and here is the

58
00:03:54,730 --> 00:03:58,340
complete pipeline. Basically, it exists. This is the CogFlow

59
00:03:58,840 --> 00:04:02,260
pipeline called Informer Pipeline, where we integrate the Informer time series

60
00:04:02,760 --> 00:04:06,700
forecasting pipeline. Step one: Preprocess data cleaning,

61
00:04:06,780 --> 00:04:10,300
data downloading, data conversion. Step two:

62
00:04:10,380 --> 00:04:14,460
Training task where we input the output

63
00:04:14,960 --> 00:04:18,300
of the preprocess task and finally use

64
00:04:18,800 --> 00:04:22,030
the train task to serve the model into

65
00:04:22,110 --> 00:04:26,470
an inference microservice. We create a CogFlow

66
00:04:26,970 --> 00:04:31,270
client and then run the full pipeline. Before, in

67
00:04:31,770 --> 00:04:35,670
the code, these individual larger functions are defined, and

68
00:04:36,170 --> 00:04:40,350
I will not go into them. Simply, to run this, we go python3,

69
00:04:40,590 --> 00:04:46,230
run_cogflowserver.py, Enter, then

70
00:04:46,730 --> 00:04:50,400
we go back to the interface, we can take a look at runs

71
00:04:50,960 --> 00:04:54,360
where now our Informer Pipeline

72
00:04:54,860 --> 00:04:59,040
has appeared and we can take a look at the pipeline. The individual

73
00:04:59,120 --> 00:05:02,680
microservices for each function are being

74
00:05:03,180 --> 00:05:06,600
spun up and executed. In a later stage we will

75
00:05:07,100 --> 00:05:10,720
see the successful execution of the pipeline. As you can see

76
00:05:10,880 --> 00:05:14,720
the individual steps, pre processing, training and serving,

77
00:05:15,220 --> 00:05:18,740
all executed in sequence. And this means now we

78
00:05:19,240 --> 00:05:22,380
have an informer living in this

79
00:05:22,880 --> 00:05:27,020
environment which is ready to serve some inference,

80
00:05:27,100 --> 00:05:30,580
for example some API request about some resource

81
00:05:31,080 --> 00:05:34,620
usage of some containers or Edge or FOX servers.

82
00:05:37,020 --> 00:05:40,460
That will be all. Thank you, I hope you enjoyed the demo.

